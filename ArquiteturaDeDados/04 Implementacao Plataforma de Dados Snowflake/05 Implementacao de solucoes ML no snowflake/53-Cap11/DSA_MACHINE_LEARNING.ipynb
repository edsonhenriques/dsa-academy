{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "lastEditStatus": {
   "notebookId": "yvjq5vxvmgmupjip5lhs",
   "authorId": "5318304106765",
   "authorName": "DANIELMCLOUD",
   "authorEmail": "danielmcloud@dsacademy.com.br",
   "sessionId": "c1136ce8-46fe-4c12-b802-bd3c3aff72d9",
   "lastEditTime": 1738630023877
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0822caf1-213c-47a5-9aa5-df6952d27d30",
   "metadata": {
    "name": "DSA01",
    "collapsed": false
   },
   "source": [
    "## Data Science Academy\n",
    "\n",
    "### Projeto e Implementação de Plataforma de Dados com Snowflake\n",
    "\n",
    "### Lab 6\n",
    "\n",
    "### Pipeline de Machine Learning com Snowpark no Snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc03d7-7e44-4dd5-940d-853bcc29e97c",
   "metadata": {
    "name": "DSA02"
   },
   "source": [
    "## Carregando Snowpark e Outros Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a5d2de-c1f7-4f5b-95e4-2be9be021498",
   "metadata": {
    "name": "DSA03",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Imports\nimport os\nimport json\nimport pickle\nimport joblib\nimport optuna\nimport logging\nimport numpy as np\nimport pandas as pd\nimport snowflake.connector\nfrom typing import List, Union, Tuple\nfrom datetime import datetime, timedelta\nfrom xgboost import XGBClassifier\nfrom dateutil.relativedelta import relativedelta\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import (confusion_matrix,\n                             accuracy_score,\n                             precision_score,\n                             recall_score,\n                             f1_score,\n                             roc_auc_score,\n                             ConfusionMatrixDisplay)\nfrom snowflake.snowpark.session import Session\nfrom snowflake.snowpark.functions import call_udf, array_construct, pandas_udf, col, udf\nfrom snowflake.snowpark import types as T\nfrom snowflake.snowpark.context import get_active_session"
  },
  {
   "cell_type": "markdown",
   "id": "a9fcda60-7a4c-458b-8d45-767dadbe3214",
   "metadata": {
    "name": "DSA04"
   },
   "source": [
    "## Criando a Sessão Snowpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ff792-350d-4bc0-8fea-83b7873292a5",
   "metadata": {
    "name": "DSA05",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Conecta na sessão ativa do Snowpark\nsession = get_active_session()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93871288-1d14-4ea7-9cb9-d1d0a8ec732e",
   "metadata": {
    "name": "DSA06",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Verifica os dados na conexão\nsession.sql(\"select current_warehouse(), current_database(), current_schema(), current_user(), current_role()\").collect()"
  },
  {
   "cell_type": "code",
   "id": "b7848163-812a-4e41-84e8-91f160c3253b",
   "metadata": {
    "language": "python",
    "name": "DSA07",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Nomes das áreas de stage\nload_data_stage = \"LOAD_DATA_STAGE\"\nmodel_stage = \"MODEL_STAGE\"\nfunction_stage = \"FUNCTION_STAGE\"\npackage_stage = \"PACKAGE_STAGE\"",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0ae669d4-bb1d-4213-a7fd-5740f48f214d",
   "metadata": {
    "name": "DSA08"
   },
   "source": [
    "## Função Para Upload de Modelo Para o Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb1111-378f-4f8b-85ff-48d078869fd5",
   "metadata": {
    "name": "DSA09",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Função para upload do modelo para o stage\ndef dsa_upload_model_to_stage(session, obj, model_stage, model_id):\n    \n    # Verifica se o nome do stage já começa com '@', se não, adiciona\n    if not model_stage.startswith(\"@\"): \n        model_stage = f\"@{model_stage}\"\n    \n    # Define o caminho temporário para salvar o arquivo antes do upload\n    temp_file_path = os.path.join(\"/tmp\", model_id)\n    \n    # Salva o objeto fornecido como um arquivo temporário usando joblib\n    joblib.dump(obj, temp_file_path)\n    \n    # Faz o upload do arquivo para o stage especificado\n    session.file.put(temp_file_path, model_stage, overwrite = True, auto_compress = False)\n    \n    # Remove o arquivo temporário após o upload\n    os.remove(temp_file_path)\n    \n    # Exibe uma mensagem indicando que o upload foi concluído com sucesso\n    print(f\"Upload do arquivo '{model_id}' finalizado com sucesso para o stage '{model_stage}'.\")"
  },
  {
   "cell_type": "markdown",
   "id": "82b0a25d-3112-43ef-ac60-19f27a7f7427",
   "metadata": {
    "name": "DSA10",
    "collapsed": false
   },
   "source": "## Função Para Deploy do Modelo Como UDF"
  },
  {
   "cell_type": "code",
   "id": "da4ad3b4-4453-46b2-9b32-57577866777d",
   "metadata": {
    "language": "python",
    "name": "DSA11",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Função de deploy\ndef dsa_deploy_model_as_udf(session: Session,\n                            model,\n                            scaler,\n                            model_id: str,\n                            model_stage: str,\n                            function_stage: str,\n                            required_packages: List[str]):\n    \n    # Define um identificador único para o scaler\n    scaler_id = f\"{model_id}_scaler\"\n    \n    # Faz o upload do scaler para o stage especificado\n    dsa_upload_model_to_stage(session, scaler, model_stage, scaler_id)\n    \n    # Faz o upload do modelo para o stage especificado\n    dsa_upload_model_to_stage(session, model, model_stage, model_id)\n\n    # Define a função de previsão que será registrada como UDF\n    def predict(features: list) -> float:\n        \n        # Converte a lista de features para um array numpy e ajusta sua forma\n        features_array = np.array(features).reshape(1, -1)\n        \n        # Aplica a transformação do scaler nos dados de entrada\n        scaled_features = scaler.transform(features_array)\n        \n        # Retorna a previsão do modelo convertida para float\n        return float(model.predict(scaled_features)[0])\n\n    # Define o nome da UDF com base no ID do modelo\n    udf_name = f\"PREDICT_{model_id}\"\n    \n    # Registra a UDF na sessão do Snowflake\n    session.udf.register(func = predict,\n                         name = udf_name,\n                         stage_location = f\"@{function_stage}\",\n                         is_permanent = True,\n                         packages = required_packages,\n                         imports = [f\"@{model_stage}/{model_id}\", f\"@{model_stage}/{scaler_id}\"])\n\n    # Exibe uma mensagem de sucesso indicando o nome da UDF criada\n    print(f\"Deploy feito com sucesso para o UDF: {udf_name}\")\n    \n    # Retorna o nome da UDF criada\n    return udf_name",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "21b75483-0373-489c-9313-15888ccee99e",
   "metadata": {
    "name": "DSA12"
   },
   "source": [
    "## Função Para Avaliar o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac4ef23-253d-47f3-a539-c6c83c7abd82",
   "metadata": {
    "name": "DSA13",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Função de avaliação do modelo\n",
    "def dsa_evaluate_model(y_test, y_pred):\n",
    "\n",
    "    # Cria a confusion matriz\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Extrai os valores\n",
    "    TN, FP, FN, TP = conf_matrix.ravel()\n",
    "\n",
    "    # Dicionário de métricas\n",
    "    metrics = {\"accuracy\": accuracy_score(y_test, y_pred),\n",
    "               \"precision\": precision_score(y_test, y_pred),\n",
    "               \"recall\": recall_score(y_test, y_pred),\n",
    "               \"f1\": f1_score(y_test, y_pred),\n",
    "               \"auc_roc\": roc_auc_score(y_test, y_pred),\n",
    "               \"TN\": TN,\n",
    "               \"FP\": FP,\n",
    "               \"FN\": FN,\n",
    "               \"TP\": TP}\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dbda87-ccf5-4151-b08d-b96396494ff0",
   "metadata": {
    "name": "DSA14"
   },
   "source": [
    "## Função Para Salvar Metadados de Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5c7e0-3b67-44be-aeb3-355b46ce193e",
   "metadata": {
    "name": "DSA15",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Função para salvar metadados de treinamento\ndef dsa_save_training_info(session,\n                           model_id,\n                           model_name,\n                           optimization,\n                           training_table,\n                           feature_columns,\n                           metrics,\n                           table_name = \"MODEL_TRAINING_INFO\"):\n    \n    # Importa a biblioteca datetime para registrar a data e hora do treinamento\n    from datetime import datetime\n\n    try:\n        # Obtém a data e hora atuais no formato string\n        training_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        # Converte a lista de colunas de features para uma string formatada corretamente\n        feature_columns_str = \",\".join([f\"'{col}'\" for col in feature_columns])\n\n        # Define a consulta SQL para inserir os detalhes do treinamento no banco de dados\n        insert_query = f\"\"\"\n                INSERT INTO {table_name} (\n                training_date,\n                model_id,\n                model_name,\n                optimization,\n                training_table,\n                feature_columns,\n                accuracy, precision, recall, f1_score, auc_roc,\n                TN, FP, FN, TP\n            )\n            SELECT \n                '{training_date}', \n                '{model_id}', \n                '{model_name}', \n                {optimization}, \n                '{training_table}', \n                ARRAY_CONSTRUCT({feature_columns_str}),\n                {metrics['accuracy']}, {metrics['precision']}, {metrics['recall']},\n                {metrics['f1']}, {metrics['auc_roc']},\n                {metrics['TN']}, {metrics['FP']}, {metrics['FN']}, {metrics['TP']};\n            \"\"\"\n\n        # Executa a consulta SQL na sessão do banco de dados\n        session.sql(insert_query).collect()\n        \n        # Exibe uma mensagem indicando que os detalhes do treinamento foram registrados com sucesso\n        print(f\"Metadados de treinamento do modelo '{model_id}' registrados com sucesso.\")\n\n    except Exception as e:\n        # Captura e exibe erros que possam ocorrer durante a execução\n        print(f\"Erro de registo para o modelo '{model_id}': {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "d49ddaab-63b7-4caa-a362-db0c722c7228",
   "metadata": {
    "name": "DSA16"
   },
   "source": [
    "## Função Para Treinamento e Deploy do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c8cdb-1a95-4cfd-bf72-6679ec8f8af0",
   "metadata": {
    "name": "DSA17",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Função de treinamento e deploy\ndef dsa_train_and_deploy_model(session: Session,\n                               model_name: str,\n                               optimize: bool,\n                               training_table: str) -> dict:\n\n    # Imports\n    import optuna\n    from sklearn.model_selection import train_test_split, cross_val_score\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.svm import SVC\n    from xgboost import XGBClassifier\n\n    # Nomes dos modelos que serão criados\n    model_abbreviations = {\"Random Forest\": \"RF\", \"XGBoost\": \"XGB\", \"K-Nearest Neighbors\": \"KNN\", \"Support Vector Machine\": \"SVM\"}\n\n    # Pacotes necessários para o treinamento\n    python_packages = [\"pandas==2.2.3\", \"scikit-learn==1.5.2\", \"xgboost==1.7.3\"]\n\n    # Valor de sequência\n    seq_value = str(session.sql(\"select MODEL_SEQ.nextval\").collect()[0][0])\n\n    # Obtém o nome do modelo\n    model_reduced = model_abbreviations.get(model_name)\n\n    # Id do modelo\n    model_id = f\"{model_reduced}_{seq_value}\"\n\n    # Id do scaler\n    scaler_id = f\"{model_reduced}_{seq_value}_scaler\"\n\n    # Define a tabela\n    df = session.table(training_table).to_pandas()\n\n    # Define x e y\n    X = df.drop(\"TARGET\", axis = 1)\n    y = df[\"TARGET\"]\n\n    # Nomes dos atributos\n    feature_columns = X.columns.to_numpy()\n\n    # Divide os dados em treino e teste\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n    # Cria o padronizador\n    scaler = StandardScaler()\n\n    # Fit e transform nos dados de treino\n    X_train_scaled = scaler.fit_transform(X_train)\n\n    # Somente em transform nos dados de teste\n    X_test_scaled = scaler.transform(X_test)\n\n    # Otimização\n    if optimize:\n\n        def objective(trial):\n\n            # Inicializa a variável\n            clf = None\n            \n            if model_reduced == \"RF\":\n\n                # Hiperparâmetros para otimização do modelo\n                params = {\"max_depth\": trial.suggest_int(\"max_depth\", 2, 64),\n                          \"n_estimators\": trial.suggest_int(\"n_estimators\", 5, 100),\n                          \"max_samples\": trial.suggest_float(\"rf_max_samples\", 0.2, 1)}\n\n                # Cria o modelo\n                clf = RandomForestClassifier(**params, random_state = 42)\n\n            elif model_reduced == \"XGB\":\n\n                # Hiperparâmetros para otimização do modelo\n                params = {\"max_depth\": trial.suggest_int(\"max_depth\", 2, 20),\n                          \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 200),\n                          \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3)}\n\n                # Cria o modelo\n                clf = XGBClassifier(**params, random_state=42)\n\n            elif model_reduced == \"KNN\":\n\n                # Hiperparâmetros para otimização do modelo\n                params = {\"n_neighbors\": trial.suggest_int(\"n_neighbors\", 1, 30)}\n\n                # Cria o modelo\n                clf = KNeighborsClassifier(**params)\n\n            elif model_reduced == \"SVM\":\n\n                # Hiperparâmetros para otimização do modelo\n                params = {\"C\": trial.suggest_float(\"C\", 1e-5, 1e5),\n                          \"gamma\": trial.suggest_float(\"gamma\", 1e-5, 1e2)}\n\n                # Cria o modelo\n                clf = SVC(**params, random_state = 42, probability = True)\n\n            else:\n                raise ValueError(\"Valor pode conter erro.\")\n\n            # Calcula o score\n            score = cross_val_score(clf, X_train_scaled, y_train, cv = 3, scoring = \"roc_auc\").mean()\n\n            # Extrai o melhor modelo\n            trial.set_user_attr(key = \"best_model\", value = clf)\n\n            return score\n\n        # Função de callback\n        def callback(study, trial):\n\n            # Verifica se já temos o melhor modelo\n            if study.best_trial.number == trial.number:\n                study.set_user_attr(key = \"best_model\", value = trial.user_attrs[\"best_model\"])\n\n        # Cria o estudo de otimização\n        study = optuna.create_study(direction = \"maximize\", sampler = optuna.samplers.RandomSampler(seed = 42))\n\n        # Executa a otimização\n        study.optimize(objective, n_trials = 100, callbacks = [callback])\n\n        # Obtém o melhor resultado\n        best_trial = study.best_trial\n\n        # Melhor modelo do melhor resultado\n        classifier = best_trial.user_attrs[\"best_model\"]\n\n    else:\n        \n        if model_reduced == \"RF\":\n            classifier = RandomForestClassifier(random_state = 42)\n        elif model_reduced == \"XGB\":\n            classifier = XGBClassifier(random_state = 42)\n        elif model_reduced == \"KNN\":\n            classifier = KNeighborsClassifier(n_neighbors = 10)\n        elif model_reduced == \"SVM\":\n            classifier = SVC(random_state = 42)\n\n    # Treina o modelo\n    classifier.fit(X_train_scaled, y_train)\n\n    # Faz a previsão\n    y_pred = classifier.predict(X_test_scaled)\n\n    # Avalia o modelo\n    metrics = dsa_evaluate_model(y_test, y_pred)\n\n    # Salva os metadados de treino\n    dsa_save_training_info(session,\n                           model_id,\n                           model_name,\n                           optimize,\n                           training_table,\n                           feature_columns,\n                           metrics)\n\n    # Deploy do UDF\n    udf_name = dsa_deploy_model_as_udf(session,\n                                       model = classifier,\n                                       scaler = scaler,\n                                       model_id = model_id,\n                                       model_stage = \"MODEL_STAGE\",\n                                       function_stage = \"FUNCTION_STAGE\",\n                                       required_packages = python_packages)\n\n    result = {\"model_id\": model_id, \"udf_name\": udf_name}\n\n    return result"
  },
  {
   "cell_type": "markdown",
   "id": "6ea7c6cb-2019-4589-9129-eaadd0e5a661",
   "metadata": {
    "name": "DSA18"
   },
   "source": [
    "## Treinamento e Deploy da Versão 1 do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e44a5-370b-4d53-bdc7-77b064129abf",
   "metadata": {
    "name": "DSA19",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Treinamento e deploy da primeira versão do modelo\ndsa_train_and_deploy_model(session, \n                           model_name = \"XGBoost\", \n                           optimize = True, \n                           training_table = \"DATA_TABLE_1\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a790f-d212-4476-827a-d3f931ae03ef",
   "metadata": {
    "name": "DSA20",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Lista o conteúdo do Model Stage\nsession.sql(\"ls @MODEL_STAGE\").collect()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf95ffe-dfdb-4e07-9bfa-8e0f9d6354f8",
   "metadata": {
    "name": "DSA21",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Converte a tabela em dataframe do pandas\ndf_training_info = session.table(\"MODEL_TRAINING_INFO\").to_pandas()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277720f-c92b-4491-8c81-6e3945b04085",
   "metadata": {
    "name": "DSA22",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Visualiza a tabela\ndf_training_info"
  },
  {
   "cell_type": "markdown",
   "id": "05b90dac-5d83-4085-8613-741a658ad6ae",
   "metadata": {
    "name": "DSA23"
   },
   "source": [
    "## Treinamento e Deploy da Versão 2 do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd218a1-dc9b-4547-8278-f68797720cf3",
   "metadata": {
    "name": "DSA24",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Treinamento e deploy da segunda versão do modelo\ndsa_train_and_deploy_model(session, \n                           model_name = \"Random Forest\", \n                           optimize = True, \n                           training_table = \"DATA_TABLE_1\")"
  },
  {
   "cell_type": "markdown",
   "id": "7e8e542a-5ccc-400b-9138-1166125b4a26",
   "metadata": {
    "name": "DSA25"
   },
   "source": [
    "## Treinamento e Deploy da Versão 3 do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751aab78-0f77-4e67-8e9b-fb1c45ab1257",
   "metadata": {
    "name": "DSA26",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Treinamento e deploy da terceira versão do modelo\ndsa_train_and_deploy_model(session, \n                           model_name = \"Support Vector Machine\", \n                           optimize = True, \n                           training_table = \"DATA_TABLE_1\")"
  },
  {
   "cell_type": "markdown",
   "id": "8cb49b13-f746-4632-a5f3-b1d9aa5e0d40",
   "metadata": {
    "name": "DSA27"
   },
   "source": [
    "## Treinamento e Deploy da Versão 4 do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1026d03-a190-4887-b0e4-6fcc34267432",
   "metadata": {
    "name": "DSA28",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Treinamento e deploy da quarta versão do modelo\ndsa_train_and_deploy_model(session, \n                           model_name = \"K-Nearest Neighbors\", \n                           optimize = True, \n                           training_table = \"DATA_TABLE_1\")"
  },
  {
   "cell_type": "code",
   "id": "9c15bc82-8739-4a8c-9e06-a34fe56c8737",
   "metadata": {
    "language": "python",
    "name": "DSA29",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Lista o conteúdo do Model Stage\nsession.sql(\"ls @MODEL_STAGE\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f364a051-5e1a-489a-8a37-d6f86d8555d3",
   "metadata": {
    "language": "python",
    "name": "DSA30",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Converte a tabela em dataframe do pandas\ndf_training_info = session.table(\"MODEL_TRAINING_INFO\").to_pandas()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b78746ec-47dd-4489-8dad-423abfb2090a",
   "metadata": {
    "language": "python",
    "name": "DSA31",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Visualiza a tabela\ndf_training_info",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f4e988e2-44ed-410f-8fe1-ed934628bb7f",
   "metadata": {
    "name": "DSA32",
    "collapsed": false
   },
   "source": "Aqui termina o Lab 6 e então iniciamos o Lab 7.\n\n## Data Science Academy\n\n### Projeto e Implementação de Plataforma de Dados com Snowflake\n\n### Lab 7\n\n### Otimização de Modelo de Machine Learning com Optuna e Inferência com Snowpark"
  },
  {
   "cell_type": "markdown",
   "id": "aa46661b-54d2-4ee2-871c-2464e426320b",
   "metadata": {
    "name": "DSA33",
    "collapsed": false
   },
   "source": "## Automatizando a Otimização de Hiperparâmetros com Optuna"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a517b6e-dfcd-4753-b600-6aa95fde800d",
   "metadata": {
    "name": "DSA34",
    "language": "python",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Path para imports do Optuna\noptuna_path = optuna.__path__[0]\n\n# Registra a função como procedure\nsession.sproc.register(func = dsa_train_and_deploy_model,\n                       name = \"dsa_train_and_deploy_model\",\n                       packages = [\"snowflake-snowpark-python\",\n                                   \"scikit-learn==1.5.2\",\n                                   \"xgboost==1.7.3\",\n                                   \"sqlalchemy==1.4.39\",\n                                   \"tqdm==4.64.1\",\n                                   \"colorlog==5.0.1\"],\n                       imports = [optuna_path],\n                       is_permanent = True,\n                       stage_location = f\"@{function_stage}\",\n                       replace = True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7fb2fa-d8cf-4c14-ab7a-64a2b32bbc7d",
   "metadata": {
    "name": "DSA35",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Parâmetros (O XGBoost é o algoritmo que mais se beneficia da otimização de hiperparâmetros entre os algoritmos que estamos usando neste Lab)\nmodel_name = \"XGBoost\"\noptimization = True\ntraining_table = \"DATA_TABLE_2\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e84b25a-e326-493e-b2c6-0de91aaa5307",
   "metadata": {
    "name": "DSA36",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Executa a procedure\nsession.call(\"dsa_train_and_deploy_model\",\n             model_name,\n             optimization,\n             training_table)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba123556-6e73-4db5-bcfe-7731addadeeb",
   "metadata": {
    "name": "DSA37",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Lista o conteúdo do Model Stage\nsession.sql(\"ls @MODEL_STAGE\").collect()"
  },
  {
   "cell_type": "code",
   "id": "879e4016-a323-4063-8e0a-0a1af274e0c4",
   "metadata": {
    "language": "python",
    "name": "DSA38",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Converte a tabela em dataframe do pandas\ndf_training_info = session.table(\"MODEL_TRAINING_INFO\").to_pandas()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a67f3948-40cd-4363-8f5d-2cff75eb831f",
   "metadata": {
    "language": "python",
    "name": "DSA39",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Visualiza a tabela\ndf_training_info",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7ce5a936-6d52-4107-9e60-3ddc4ab22ed5",
   "metadata": {
    "name": "DSA40"
   },
   "source": [
    "## Função Para Salvar Metadados da Inferência do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3ead17-ca81-4cdf-afb7-1c2df9237ccb",
   "metadata": {
    "name": "DSA41",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Função para salvar metadados da inferência\ndef dsa_save_inference_details(session,\n                               model_id,\n                               training_table,\n                               test_table,\n                               metrics,\n                               table_name = \"INFERENCE_RESULTS\"):\n\n    # Importa a classe datetime para manipular datas\n    from datetime import datetime\n\n    try:\n        \n        # Obtém a data e hora atual formatada\n        inference_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        # Monta a query SQL para inserir os metadados da inferência na tabela especificada\n        insert_query = f\"\"\"\n                INSERT INTO {table_name} (\n                    inference_date,\n                    model_id,\n                    training_table,\n                    test_table,\n                    accuracy, precision, recall, f1_score, auc_roc,\n                    TN, FP, FN, TP\n                )\n                VALUES (\n                    '{inference_date}',\n                    '{model_id}',\n                    '{training_table}',\n                    '{test_table}',\n                    {metrics['accuracy']}, {metrics['precision']}, {metrics['recall']},\n                    {metrics['f1']}, {metrics['auc_roc']},\n                    {metrics['TN']}, {metrics['FP']}, {metrics['FN']}, {metrics['TP']}\n                );\n                \"\"\"\n\n        # Executa a query no banco de dados\n        session.sql(insert_query).collect()\n        \n        # Exibe mensagem de sucesso\n        print(f\"Metadados de inferencia salvos com sucesso para o modelo {model_id}.\")\n\n    except Exception as e:\n        # Exibe mensagem de erro caso ocorra alguma exceção\n        print(f\"Erro ao salvar metadados de inferência: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "3c468082-a70a-443a-897c-b21ea4b72da3",
   "metadata": {
    "name": "DSA42"
   },
   "source": [
    "## Função Para Inferência com o Modelo Treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968e8b97-cf6f-4081-b941-f904eddf6cc2",
   "metadata": {
    "name": "DSA43",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Função de inferência\ndef dsa_run_inference(session: Session,\n                      test_table: str,\n                      model_id: str,\n                      predictions_table: str = \"PREDICTIONS_RESULT\",\n                      target_column: str = \"TARGET\") -> None:\n    \n    try:\n        \n        # Consulta informações do modelo treinado no banco de dados\n        query_result = session.sql(\n            f\"\"\"\n            SELECT FEATURE_COLUMNS, TRAINING_TABLE\n            FROM MODEL_TRAINING_INFO \n            WHERE MODEL_ID = '{model_id}'\n            \"\"\"\n        ).collect()\n\n        # Obtém a tabela usada para treinamento do modelo\n        training_table = query_result[0][\"TRAINING_TABLE\"]\n        \n        # Obtém os nomes das colunas de características utilizadas no modelo\n        feature_columns = query_result[0][\"FEATURE_COLUMNS\"]\n        \n        # Converte a string JSON contendo os nomes das colunas para uma lista\n        feature_columns = json.loads(feature_columns)\n        \n        # Cria um array com as colunas de características para a inferência\n        features_array = array_construct(*[col(c) for c in feature_columns])\n\n        # Carrega os dados de teste na sessão\n        df = session.table(test_table)\n        \n        # Verifica se a coluna alvo está presente nos dados de teste\n        has_target = target_column in df.columns\n\n        # Seleciona as colunas de características e a coluna alvo (se existir), \n        # aplicando a função de predição do modelo\n        df_predictions = df.select(\n            *feature_columns,\n            *([target_column] if has_target else []),\n            call_udf(f\"PREDICT_{model_id}\", features_array).alias(\"PREDICTION\"),\n        )\n\n        # Salva os resultados das previsões na tabela de saída\n        df_predictions.write.mode(\"overwrite\").save_as_table(predictions_table)\n        \n        # Exibe uma mensagem informando que as previsões foram salvas com sucesso\n        print(f\"Resultados salvos na tabela {predictions_table}!\")\n\n        # Caso a coluna alvo esteja presente, calcula métricas de avaliação do modelo\n        if has_target:\n            \n            # Converte os resultados das previsões para um DataFrame Pandas\n            df_predictions_pd = df_predictions.to_pandas()\n            \n            # Calcula métricas de avaliação comparando previsões e valores reais\n            metrics = dsa_evaluate_model(df_predictions_pd[target_column], \n                                         df_predictions_pd[\"PREDICTION\"])\n\n            # Salva os detalhes da inferência para futuras análises\n            dsa_save_inference_details(session,\n                                       model_id,\n                                       training_table,\n                                       test_table,\n                                       metrics)\n\n        return metrics\n\n    except Exception as e:\n        # Lança uma exceção em caso de erro durante a inferência\n        raise Exception(f\"Erro ao executar a inferência: {str(e)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "b8483ae7-f3de-4b25-82de-8c3d2f36d60c",
   "metadata": {
    "name": "DSA44"
   },
   "source": [
    "## Executando a Inferência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc91363-fcb8-4f78-8583-057f8613dbd6",
   "metadata": {
    "name": "DSA45",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Testando a inferência com um dos modelos\ndsa_run_inference(session, test_table = \"DATA_TABLE_2\", model_id = \"RF_2\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2951a4b-f532-480f-9e75-424860ed76a6",
   "metadata": {
    "name": "DSA46",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Registra a função como procedure para automação\nsession.sproc.register(func = dsa_run_inference,\n                       name = \"dsa_run_inference\",\n                       packages = [\"snowflake-snowpark-python\",\n                                   \"scikit-learn==1.2.1\",\n                                   \"joblib==1.1.1\",\n                                   \"xgboost==1.7.3\"],\n                       is_permanent = True,\n                       stage_location = f\"@{function_stage}\",\n                       replace = True)"
  },
  {
   "cell_type": "code",
   "id": "96c23df9-fdc4-4b5f-aff1-2b6e01c8f080",
   "metadata": {
    "language": "python",
    "name": "DSA47",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Lista o conteúdo do Model Stage\nsession.sql(\"ls @MODEL_STAGE\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d1640f2c-6197-4fa6-a890-7db4e39a17f3",
   "metadata": {
    "language": "python",
    "name": "DSA48",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Executa a procedure\nsession.call(\"dsa_run_inference\", \"DATA_TABLE_3\", \"XGB_5\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44562aa3-6cd6-4ba3-94b0-7e2ce4a9e3f8",
   "metadata": {
    "name": "DSA49",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Executa a procedure\nsession.call(\"dsa_run_inference\", \"DATA_TABLE_3\", \"RF_2\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c4c4d-e817-4a94-abc1-55acfb8cff3b",
   "metadata": {
    "name": "DSA50",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Executa a procedure\nsession.call(\"dsa_run_inference\", \"DATA_TABLE_3\", \"SVM_3\")"
  },
  {
   "cell_type": "code",
   "id": "9b2e6a14-57b3-4d75-aa53-35a69b1cf15e",
   "metadata": {
    "language": "python",
    "name": "DSA51",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Executa a procedure\nsession.call(\"dsa_run_inference\", \"DATA_TABLE_3\", \"KNN_4\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "70790b50-418a-4ea8-9453-837186a4739b",
   "metadata": {
    "name": "DSA52"
   },
   "source": [
    "## Visualizando as Previsões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0efcd4c-25bc-4dad-8f6c-16a9d6a01318",
   "metadata": {
    "name": "DSA53",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Converte para dataframe do pandas\ndf_predictions_result = session.table(\"PREDICTIONS_RESULT\").to_pandas()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5627e0-20fb-4d98-b821-1208f90faa4c",
   "metadata": {
    "name": "DSA54",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Visualiza a tabela de previsões\ndf_predictions_result"
  },
  {
   "cell_type": "markdown",
   "id": "cbff2cb1-df2f-420a-8784-21a97a902903",
   "metadata": {
    "name": "DSA55",
    "collapsed": false
   },
   "source": [
    "# Fim"
   ]
  }
 ]
}