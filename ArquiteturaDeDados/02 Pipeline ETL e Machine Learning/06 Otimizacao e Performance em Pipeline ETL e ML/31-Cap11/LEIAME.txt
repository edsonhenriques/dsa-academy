# Instruções para executar o Projeto 5

# 1- Adicione a linha abaixo no arquivo requirements.txt do cluster Spark:

pendulum==3.0.0

# 2 - Coloque o arquivo dataset.csv na pasta de dados do cluster Spark.

# 3 - Coloque os scripts PySpark no pasta de jobs do cluster Spark.

# 4 - Coloque os arquivos da pasta yarn dentro da pasta yarn do cluster Spark.

# 5 - Recrie o cluster a partir do zero. Estamos considerando o cluster do Capítulo 6.

# 6- Execute o comando abaixo no terminal ou prompt de comando (Vamos usar o deploy mode como client para reduzir o consumo de memória RAM):

docker exec dsa-spark-master-yarn spark-submit --deploy-mode client ./apps/projeto5.py


Para analisar o resultado:

docker exec dsa-spark-master-yarn hdfs dfs -ls /

docker exec dsa-spark-master-yarn hdfs dfs -ls /opt

docker exec dsa-spark-master-yarn hdfs dfs -ls /opt/spark

docker exec dsa-spark-master-yarn hdfs dfs -ls /opt/spark/data

docker exec dsa-spark-master-yarn hdfs dfs -ls /opt/spark/data/modelos

docker exec dsa-spark-master-yarn hdfs dfs -ls /opt/spark/data/modelos/bestModel